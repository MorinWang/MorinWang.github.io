---
layout: post
title: A Small Scope of Fairness Problems in Machine Learning
tags: Fairness-learning
stickie: True
---

------

#### A. Introduction:

Recently I read a news about "Quicken Loans". It said that "an Iowa State University study published the same year found that LGBTQ couples were 73% more likely to be denied a mortgage than heterosexual couples with comparable financial credentials in USA." 

In practice, most banks in USA rely on the credit scoring models and these models seem to be a racist to some extend. So if the attitude towards sex or the colors of skin influence the judgment of models, such systemic inequality would cause some significant gaps between different human groups. (Such as the significant increase in the Gini coefficient)

##### A.1 Why fair？

"We hold these truths to be self-evident, that all men are created equal."

##### A.2 Unfair reasons

Machine Learning(ML) algorithms are to make corresponding models can predict the unknown situation best via training data. It seems that ML models can make the most proper and impersonal judgment. So , What may cause the unfairness in Machine Learning models?  There are some potential reasons:

###### a. Data Bias.

Data is collected by human, the training dataset in ML models.

Example:

Prediction in crime: The data about arrested people is used to predict the crime. But we don't have the true crime data. Police density (drug crimes are policed at a higher rate toward minority races due to the prejudice)  

###### b. Tend to fit majority population.

Different Groups have different distributions over features. The models will consider more about the majority (because the accuracy and the loss would be lowered.)

Example:

SAT group A: Employ tutors and take multiple tests.(Majority)

SAT group B: Depend on themselves and take the test once.(Minority)

We would naturally expect average SAT scores in A higher than that in B.

So, if wo train a group blind model to predict the academic performance, it may be a little unfair to group B.

*This can be alleviated via concerted data gathering effort. [1]

###### c. The need to do exploration.

In some important problems, we should explore more about the environment(sometimes we should do something sub-optimal) to gather more data.  Such collecting may be unfair.

Example:

Drug prediction: We only observe the drug efficacy when we assign it to patients. If we have a good drug but we want to test a potential better drugs, it may be unfair for tested patients.

#### B. Problems:

##### B.1 How to evaluate the fairness of models(to try to solve or alleviate it)?

Solution: Make mathematical definition. (Put the definition as a regularization.)

###### a. statistical definition

Most paper focus on this definition. It is:

- Fix protected demographic groups G. (Like sexual attitude or race)
- Define a statistical measure across all of these groups. (Measure can be different)

Advantage:

- Simple. No more assumption is needed and fairness can be can be easily verified. 

Disadvantage:

- No guarantees to individuals or subgroups. (Consider "average” members of the protected groups). [2]
- Different statistical measures of fairness can be at odds with one another. [3]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             

###### b. Individual definition

- Constraints that bind on specific pairs of individuals. 
- “Similar individuals should be treated similarly.” [2]
- “Less qualified individuals should not be favored over more qualified individuals" [4]

Advantage:

- Meaningful and flexible.

Disadvantage:

- Require making significant assumptions. So individual defined fairness may not be made very practical. (For example: [2] assume an agreed upon similarity metric. [4] assume functional form of the relationship between features and labels.)

##### B.2 What can we do?(Some questions)

###### a. Make statistical definition better

[7] try to make the statistical definition not only to hold not just on a small number of protected groups, but on an exponential or infinite class of groups defined by some class of functions of bounded complexity. 

###### b. Weaken assumptions on Individual definition

"The algorithm designer has perfect knowledge of a “fairness metric."[2] should be weaken.

[6] assumed the algorithm has access to an oracle which can return an unbiased estimator for the distance between two randomly drawn individuals according to an unknown fairness metric. Such oracle can also meet the statistical  definition.

###### c. Online learning with an unknown fairness metric

[8] assumed the existence of an oracle which can identify fairness **violations** when they are made in an online setting but cannot quantify the extent of the violation. And they proved when the metric is from a specific learnable family, this kind of feedback is sufficient to obtain an **optimal regret bound** to the best fair classifier.

But these fairness is defined with respect to some metric(oracles).

###### d. Dynamic fairness.

Actions of the learning algorithm will cause a complex environments that are dynamically changing.

Example: 

**Individual Fairness Confirmed -> Composed Fairness  will be Confirmed?**(Individual and Composed)

[9] proved that fairness under composition and find that often the composition of multiple fair 		components will not satisfy any fairness constraint at all.  And the individual components of a fair 			system may appear to be unfair in isolation. 

**Academic preference of some groups and their next generations?** (Environments)

Perhaps we can model the environment as a Markov decision process.

 [10] considering the equilibrium effects of imposing statistical definitions of fairness in a model of a labor market.

###### d. Correcting bias.

The trained embeddings on a corpus of Google News texts (word2vec)[7] have  some prejudgments. For example, “doctor” is more similar to man than to woman, along with analogies such as “man is to computer programmer as woman is to homemaker.” It might result in male applicants being ranked more highly than equally qualified female applicants in queries related to jobs.

**How to solve it?**

- **Bias correction**

  - ​	Knowledge of how the measurement process is biased, 
  - ​	Properties the data that would satisfy the an “unbiased” world.

  Friedler [11] proposed a disconnect between the observed space and the unobservable construct space  Data correction is to undo the effects of bias between these spaces. (Mapping) But it is hard.

- When can the mapping between the construct space and the observed space be learned and inverted? 

- What form of fairness does the correction promote, and at what cost?

- **Fair Representation**

  Transformations (intermediate representations) of the original data that **retain** task-relevant **information** while **removing** information about **sensitive** attributes. 

  - [12] Fair representation learning produces a debiased data set that may in principle be used by other parties without any risk of disparate outcomes. 
  - Feldman[13] formalize this idea by showing how the disparate impact of a decision rule is bounded in terms of its balanced error rate as a predictor of the sensitive attribute.
    - Feldman[13] propose rank-preserving procedures for **repairing features** to reduce or remove pairwise dependence with the protected attribute. 
    - Johndrow [14] build upon this work, introducing a **likelihood-based approach** that can additionally handle continuous protected attributes, discrete features .

##### B.3 Reinforcement Learning

[4] consider the fairness in contextual multi-armed bandit problems. The quality of a the KWIK(Knows What It Knows) learning algorithm in this problem is characterized by its “KWIK bound”, they proved that the optimal learning rate of any fair algorithm is determined by the best KWIK bound. They also showed a classical KWIK  learning algorithm can be converted into a fair contextual bandit algorithm.

[5]showed the unfairness in the usage of importance sampling in the off-policy policy selection problem. They defined the fairness as follows: "With any amount of historical data, the probability that the algorithm selects a policy with the largest value should be greater than the probability that it selects a policy that does not have the largest value." (I believed that this might be a induvial definition.) They proposed the new way to tackle this issue.

#### C. Table Format:

| Year Index               | Experiments                                                  | Problems to Solve                                            | Fair definition                                              | Contributions                                                |
| ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2018 [1]                 | Adult dataset                                                | Use Bias-variance-noise decompositions of discrimination level to prove their arguments and propose that **increasing training set size** and **measuring additional variables** are two ways to reduce discriminations. | Statistical Notion                                           | 1. Some kinds of unfairness  could be addressed through data collection. <br />2. The use of clustering for identifying subpopulations |
| 2021(Latest Version) [2] | *No*                                                         | “Fairness on average” towards the entire protected group  is insufficient and could be seriously abused. (**Statistical is not sufficient**) They proposed a similarity metric based individual definition. | Individual                                                   | They  give conditions on the similarity metric to ensure fairness for individuals (the Lipschitz condition) yields group fairness . |
| 2017 [3]                 | Broward County data (Crime prediction)                       | To discuss several statistical  fairness criteria.           | Statistical                                                  | Predictive parity cannot have equal false positive and negative rates across groups when the recidivism prevalence differs across those groups. |
| 2016 [4]                 | *No*                                                         | The study of fairness in multi-armed bandit problems. (The made a individual definition:" A worse applicant is never favored over a better one, despite a learning algorithm’s uncertainty over the true payoffs") | Individual                                                   | They proved that  a KWIK (Knows What It Knows) learning model for a class of functions can be transformed into a provably fair contextual bandit algorithm and vice versa. |
| 2017[5]                  | A simple policy selection.                                   | Classical importance sampling in the off-policy policy selection problem  is unfair. | Individual(I guess)                                          | They  showed the unfairness in the important sampling problems and also proposed a theoretical solution and a new practical estimator to solve the problems. |
| 2017[6]                  | No                                                           | The similarity metric should be known for all pairs of individuals in previous papers. | New. More like individual ( their well defined metric multi-fairness can also meet the statistical  definition.) | Multi-fairness: similar **subpopulations** are treated similarly. (Achieved by the average across the group)<br />Their framework does not assume the entire metric is known to the learning algorithm. |
| 2018[7]                  | Communities and Crime dataset(Crime prediction)              | 1. The auditing binary classifiers for equal opportunity and statistical parity.<br />2. Learning classifiers subject to  constraints, when the number of protected groups is large. | New. More like statistical.                                  | The subgroup can be divided in any way. So there should be a huge number of subgroups. Their definition make the statistical definition not only to hold not just on a small number of protected groups, but on an exponential or infinite class of groups defined by some class of functions of bounded complexity. |
| 2018[8]                  | No                                                           | They want to investigate the extent to which it is possible to satisfy individual fairness constraint while simultaneously solving an online learning problem. (The whole metric is not known to models.) | Individual                                                   | The assumed that the algorithm has access to an oracle that knows intuitively what it means to be fair, but cannot explicitly enunciate the fairness metric.<br />They proved that if the metric is from a specific learnable family, it is sufficient to obtain an **optimal regret bound** to the best fair classifier. |
| 2017[9]                  | A toy example. (Not real, a situation about interviewed and hired) | The want to study the fairness in the decision-making processes. | Statistical                                                  | They studied the fairness problems in a compound decision-making processes. They showed that the composition of fair components may not guarantee a fair decision-making process( they called the process "pipeline". ) |
| 2017[10]                 | No                                                           | The current group disparate outcomes may be immovable **even** when hiring decisions are bound by an input-output notion of “individual fairness.” ( Individual fairness is not very good for labor decision problems) | They discussed both.(But statistical in their model)         | They constructed a dynamic reputational model of the labor market that illustrates the reinforcing nature of asymmetric outcomes resulting from groups’ divergent access to resources and investment choices. |
| 2016[11]                 | No                                                           | "To study algorithmic fairness is to study the interactions between different spaces that make up the decision pipeline for a task." | Individual                                                   | They introduced the notion of a construct space and proposed a disconnect between the observed space and the unobservable construct space. , They showed that fairness can be guaranteed only with very strong assumptions about the world. |
| 2013[12]                 | Dataset: German, Adult, and Health                           | 1. Metric needed. It is not very realistic <br />2.Unseen data cannot be generalized | Both                                                         | They formulate fairness as an optimization problem of finding a good representation.<br />A good representation: encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group. |
| 2015[13]                 | Ricci dataset(about exam)<br />German credit<br />Adult income | Identifying and removing disparate impact.                   | More like Statistical                                        | They proposed **rank-preserving procedures** for repairing features to reduce or remove pairwise dependence with the protected attribute. |
| 2019[14]                 | A ProPublica dataset related to the criminal justice system in Broward County, Florida. | A probabilistic notion of algorithmic bias.                  | More like Statistical                                        | They introduced approach that can additionally handle continuous protected attributes and discrete features. Their method allows the user to make adjustments such that the output dataset is mutually independent of the protected variables. |
| 2018[15]                 | MovieLens and Twitter                                        | The fairness in recommendation via Tensor Decomposition      | Statistical                                                  | Evaluate models via absolute difference between different groups |



#### D. Paragraph & Discussion:

##### Relationship with TD:

This paper proposed that tensor decompositions should also be fair in the application of recommendation. This paper isolate the sensitive information in the framework of Tensor decompositions.

Fair definition: Statistical. "The preference rating is independent of the sensitive attribute." 

That is: P[R] = P[R\|S]

- The problem becomes:

![GitHub 1](image0.png)
  Distract the sensitive information via A'' an then this information can be removed.

![GitHub 2](image1.png)

![GitHub 3](image2.png)



- The evaluation aspects: 
  - **Recommendation quality,**
  - **Recommendation fairness** (absolute difference between mean ratings of different groups), 
  - **Effectiveness of eliminating sensitive information **(the sum of absolute cosine angles between non-sensitive and sensitive dimensions)



###### References:

[1] Chen, Irene, Fredrik D. Johansson, and David Sontag. "Why is my classifier discriminatory?." *Advances in Neural Information Processing Systems*. 2018.

[2] Dwork C, Hardt M, Pitassi T, et al. Fairness through awareness[C]//Proceedings of the 3rd innovations in theoretical computer science conference. 2012: 214-226.

[3] Chouldechova, Alexandra. "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments." *Big data* 5.2 (2017): 153-163.

[4] Joseph, Matthew, et al. "Fairness in learning: Classic and contextual bandits." *Advances in neural information processing systems* 29 (2016): 325-333.

[5] Doroudi, Shayan, Philip S. Thomas, and Emma Brunskill. "Importance Sampling for Fair Policy Selection." *Grantee Submission* (2017).

[6] Kim, Michael P., Omer Reingold, and Guy N. Rothblum. "Fairness through computationally-bounded awareness." *arXiv preprint arXiv:1803.03239* (2018).

[7] Kearns, Michael, et al. "Preventing fairness gerrymandering: Auditing and learning for subgroup fairness." *International Conference on Machine Learning*. PMLR, 2018.

[8] Gillen, Stephen, et al. "Online learning with an unknown fairness metric." Advances in Neural Information Processing Systems, arXiv:1802.06936 (2018).

[9] Bower, Amanda , et al. "Fair Pipelines." (2017).

[10] Hu, Lily , and Y. Chen . "A Short-term Intervention for Long-term Fairness in the Labor Market." (2017).

[11] Friedler, Sorelle A , C. Scheidegger , and S. Venkatasubramanian . "On the (im)possibility of fairness." (2016).

[12] Zemel, Rich, et al. "Learning fair representations." *International conference on machine learning*. PMLR, 2013.

[13] Feldman, Michael, et al. "Certifying and removing disparate impact." *proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining*. 2015.

[14]  Johndrow, James E., and Kristian Lum. "An algorithm for removing sensitive information: application to race-independent recidivism prediction." *The Annals of Applied Statistics* 13.1 (2019): 189-220.

[15] Zhu, Ziwei, Xia Hu, and James Caverlee. "Fairness-aware tensor-based recommendation." *Proceedings of the 27th ACM International Conference on Information and Knowledge Management*. 2018.

[16] Chouldechova, Alexandra , and A. L. Roth . "A snapshot of the frontiers of fairness in machine learning." *Communications of the ACM* (2020).